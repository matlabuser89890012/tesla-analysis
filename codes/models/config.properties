# TorchServe Configuration

# Inference API Port
inference_address=http://0.0.0.0:8080

# Management API Port
management_address=http://0.0.0.0:8081

# Metrics API Port
metrics_address=http://0.0.0.0:8082

# Number of worker threads
number_of_netty_threads=4

# Maximum number of workers per model
job_queue_size=100

# Model store location
model_store=/home/model-server/model-store

# Load models at startup
load_models=tsla=tsla-model.mar

# Enable GPU
number_of_gpu=1
